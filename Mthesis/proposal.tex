\documentclass{article}

\usepackage{array}


\usepackage{amsmath,amssymb,amsfonts}

\newcommand{\B}{\mathbb{B}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rc}{\widehat{\R}}
\newcommand{\Rp}{\R^+}
\newcommand{\Rn}{{\displaystyle \R^{\mkern-2mu n}}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Zp}{\Z^+}
\newcommand{\Zn}{{\displaystyle \Z^n}}
\newcommand{\ve}{\varepsilon}

\newcommand{\transpose}[1]{#1^{\mathrm{T}}}

\newcommand{\Epil}{\quad\Longleftrightarrow\quad}
\newcommand{\Fpil}{\longrightarrow}
\newcommand{\Ipil}{\quad\Longrightarrow\quad}
\newcommand{\Lpil}{\rightarrow}

\newcommand{\mc}{\mathcal}
\newcommand{\vek}{\mathbf}

\newcommand{\cupd}{\mathbin{\dot{{\cup}}}}

\newcommand{\UOne}{\mathrm{U}^{\vek{1}}}
\newcommand{\re}{\mathop{\mathrm{Re}}}
\newcommand{\im}{\mathop{\mathrm{Im}}}
\newcommand{\Tr}{\mathop{\mathrm{Tr}}}

\makeatletter
\newcommand*{\Norm}[2][\@gobble]{\left\|#1. #2 \right\|}
\newcommand*{\norm}[2][\@gobble]{\left|#1. #2 \right|}
\newcommand*{\setOf}[3][\@gobble]{%
   \left\{ \, #2 \,\,\vrule\relax#1.\,\, #3 \, \right\}%
}
\makeatother


\usepackage{amsthm}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\providecommand*{\Dash}{%
   \nobreak\hspace{0.166667em}\textemdash\hspace{0.166667em}%
}
\providecommand*{\Ldash}{%
   \hspace{0.166667em}\textemdash\nobreak\hspace{0.166667em}%
}
\providecommand*{\Rdash}{\Dash}
\providecommand*{\dash}{\textendash\hspace{0pt}}


\providecommand*{\DefOrd}[2][]{\textbf{#2}}
\providecommand*{\emDefOrd}[2][]{\emph{#2}}


\usepackage{sdpgf}

\begin{document}

\title{A Unitary Heuristic for the Least-Squares Optimal Network 
  Layout Problem}
\author{Lars Hellstr\"om}
\maketitle

\begin{abstract}
  The goal in this work is to implement and evaluate a particular 
  heuristic for a mixed (discrete--continuous) optimisation problem: 
  how often does it find the optimum, how good is the solution it 
  finds, and how sensitive is it to the initial guess?
  
  The heuristic in question is to relax the discrete variables in the 
  problem (which can be formalised as permutations) into elements of 
  a continuous matrix group, seeking the optimum for this continuous 
  problem, and finally seeking a nearby solution to the original 
  mixed problem by adding a penalty potiential to force the matrices 
  back to being permutations. For topological reasons, the matrix 
  group used should probably be the unitary group (or a subgroup 
  thereof).
  
  The original optimisation problem is a special case of the graph 
  layout problem, for a class (the \emph{networks}) of Directed Acyclic 
  Graphs where each vertex has been assigned a level but it remains 
  to arrange the vertices within each level so that the overall 
  layout is as clear as possible. While clarity is probably a 
  subjective concept that defies complete formalisation, minimising 
  the lengths of the edges should at least help to prevent many 
  trivial ways in which the layout can be obfuscated, and also reduce 
  sensations of ``but that should be drawn in \emph{the other way} 
  instead'' considerably. Hence it is interesting to seek a layout 
  that will minimise the sum of the squares of the lengths of the 
  edges: a least-squares optimal layout.
  Obviously, it is because this target function has such a nice 
  form that the unitary matrix relaxation becomes feasible.
  
  The problem of finding a layout for networks is of interest because 
  networks are a natural form for expressions in algebraic theories 
  that mix operations and co-operations. In this capacity as 
  expressions, a very large number of networks are being generated by 
  a program of the author for completion of network rewriting 
  systems. Presenting these calculations require generating a layout 
  for each network involved. 
%   Several methods for this have been 
%   implemented, but only the one currently in production use produces 
%   acceptable results, and it can still makes choices that seem 
%   unnatural for a human. Minimising combined edge length should help 
%   to address that.
\end{abstract}


\section{Details}

\begin{definition}
  An \DefOrd{$\N^2$-graded set} (also known as a \emDefOrd{doubly 
  ranked} set) $\mc{P}$ is a set together with two 
  functions \(\alpha_{\mc{P}},\omega_{\mc{P}}\colon \mc{P} \Fpil \N\). 
  In this context, the value $\alpha_{\mc{P}}(b)$ is called the 
  \DefOrd{arity} of $b$ and the value $\omega_{\mc{P}}(b)$ is called 
  the \DefOrd{coarity} of $b$; the dependence on $\mc{P}$ is 
  usually elided. Denote by $\mc{P}(m,n)$ the set of those elements in 
  $\mc{P}$ which have coarity $m$ and arity $n$.
\end{definition}

\begin{definition} \label{Def:Network}
  A \DefOrd{network} of an $\N^2$-graded set $\mc{P}$ is a tuple
  \begin{equation}
    G = (V,E,h,g,t,s,D)
  \end{equation}
  where
  \begin{itemize}
    \item
      $V$ is a finite set, called the set of \emDefOrd{vertices}. \(V 
      \owns 0,1\), where $0$ is called the \emDefOrd{output vertex} 
      and $1$ is called the \emDefOrd{input vertex}. Elements of \(V 
      \setminus \{0,1\}\) are called \emDefOrd{inner vertices}.
    \item
      $E$ is a finite set, called the set of \emDefOrd{edges}.
    \item
      \(h\colon E \Fpil V \setminus \{1\}\) is called the 
      \emDefOrd{head} map. \(t\colon E \Fpil V \setminus \{0\}\) is 
      called the \emDefOrd{tail} map. Define the input $d^-(v)$ and 
      output $d^+(v)$ valencies of a vertex $v$ by
      \begin{align*}
        d^-(v) ={}& 
          \norm[\Big]{ \setOf[\big]{ e \in E }{ h(e) = v } }
          \text{,}\\
        d^+(v) ={}& 
          \norm[\Big]{ \setOf[\big]{ e \in E }{ t(e) = v } }
          \text{.}
      \end{align*}
    \item
      \(g,s\colon E \Fpil \Zp\) are 
      called the \emDefOrd{head index} and \emDefOrd{tail index} 
      maps.
    \item
      \(D\colon V \setminus \{0,1\} \Fpil \mc{P}\) is called the 
      \emDefOrd{annotation}.
  \end{itemize}
  and these satisfy
  \begin{enumerate}
    \item \label{A1:Network}
      The tuple $(V,E,h,t)$ is a directed acyclic graph, i.e., there 
      is no sequence \(e_1,\dotsc,e_n \in E\) such that 
      \(h(e_i)=t(e_{i+1})\) for \(i=1,\dotsc,n-1\) and 
      \(h(e_n)=t(e_1)\).
    \item \label{A2:Network}
      Head and head index uniquely identifies an edge, as does the 
      combination of tail and tail index; if \(e_1,e_2 \in E\) satisfy 
      \(h(e_1)=h(e_2)\) and \(g(e_1) = g(e_2)\) then \(e_1=e_2\); 
      similarly if \(e_1,e_2 \in E\) satisfy \(t(e_1)=t(e_2)\) and 
      \(s(e_1) = s(e_2)\) then \(e_1=e_2\).
    \item \label{A3:Network}
      Head indices are assigned from $1$ and up; if \(e_1 \in E\) is 
      such that \(g(e_1)>1\) then there exists some \(e_2 \in E\) such 
      that \(h(e_2)=h(e_1)\) and \(g(e_2) = g(e_1)-1\). Similarly 
      tail indices are assigned from $1$ and up; if \(e_1 \in E\) is 
      such that \(s(e_1)>1\) then there exists some \(e_2 \in E\) such 
      that \(t(e_2)=t(e_1)\) and \(s(e_2) = s(e_1)-1\).
    \item \label{A4:Network}
      Arities and coarities of the annotations agree with the 
      in-valencies and out-valencies respectively of the inner vertices; 
      \(d^-(v) = \alpha\bigl( D(v) \bigr)\) and 
      \(d^+(v) = \omega\bigl( D(v) \bigr)\) for every \(v \in V 
      \setminus \{0,1\}\).
  \end{enumerate}
  The arity $\alpha(G)$ is defined to be the valency $d^+(1)$ of the 
  input vertex $1$ and coarity $\omega(G)$ is defined to be the 
  valency \(d^-(0)\) of the output vertex $0$.
%   Denote by $\Nw(\mc{P})$ the $\N^2$-graded set of all networks
%   of $\mc{P}$ which have \(V,E \subset \N\).
\end{definition}


\begin{definition}
  Let $G=(V,E,h,g,t,s,D)$ be a network. An \DefOrd{item} in $G$ is an 
  element of $\bigl( V \setminus\nobreak \{0,1\} \bigr) \cupd E$, i.e., 
  \emph{either} an edge or an inner vertex.
  % Denote by $I(G)$ the set of all items of $G$.
  
  A \DefOrd{gradation} $\ell$ of $G$ is a function \(V \Fpil \Z\) such 
  that \(\ell\bigl( h(e) \bigr) < \ell\bigl( t(e) \bigr) \) for all 
  \(e \in E\), \(\ell(0) < \ell(v)\) for all \(v \in V\setminus\{0\}\), 
  and \(\ell(1) > \ell(v)\) for all \(v \in V\setminus\{1\}\). A 
  \DefOrd{graded network} is a pair $(G,\ell)$ where $G$ is a network 
  and $\ell$ is a gradation of $G$. A \DefOrd{link} in the graded 
  network $(G,\ell)$ is a pair \((e,k) \in E \times \Z\) where 
  \(\ell\bigl( h(e) \bigr) \leqslant k\) and \(k < \ell\bigl( t(e) 
  \bigr)\). Denote by $Q(G,\ell)$ the set of all links of $(G,\ell)$.
  
  \DefOrd[*{level}]{Level} $k$ in the graded network $(G,\ell)$ is 
  defined to be the set
  \begin{equation*}
    L_k = \begin{cases}
      \setOf[\big]{ e \in E }{ h(e) = 0 }& \text{if \(k=\ell(0)\),}\\
      \setOf[\big]{ e \in E }{ t(e) = 1 }& \text{if \(k=\ell(1)\),}\\
      \setOf[\big]{ v \in V }{ \ell(v) = k } \cupd
        \setOf[\Big]{ e \in E }{ \ell\bigl( h(e) \bigr) < k
          \text{ and } k < \ell\bigl( t(e) \bigr) }
        & \text{otherwise}
     \end{cases}
  \end{equation*}
  of items of $G$. A link $(e,k)$ should be though of as connecting 
  a head item ($h(e)$ or $e$) in $L_k$ to a tail item ($t(e)$ or $e$) in 
  $L_{k+1}$. 
\end{definition}

\begin{figure}
  \tabskip=0pt plus 1fil
  \halign to \linewidth{\hfil#\hfil&\hfil#\hfil\cr
%   \begin{tabular*}{\linewidth}{@{\extracolsep{0pt plus 1 fil}}c c}
    \begin{sdpgf}{0}{0}{75}{-170}{1.0pt}
      \m 23 -59 \C 0 8 -4 9 5 5 \S \m 58 -133 \C 7 6 3 10 0 9 \L 0 10
      \C 0 12 -23 2 0 12 \L 0 10 \C 0 9 -3 10 -6 6 \S \m 30 -23 \L 0
      20 \S \m 51 -97 \C 6 6 3 10 0 9 \L 0 10 \C 0 9 -7 8 0 9 \L 0 10
      \C 0 8 -8 7 0 8 \S \m 15 -95 \C 0 8 -4 9 6 5 \S \m 39 -97 \C -6
      6 2 12 -7 6 \S \m 47 -133 \C -9 9 -23 -1 0 14 \S \m 23 -131 \C 0
      11 22 -2 0 11 \S \m 30 -168 \C 0 8 -7 6 0 7 \S \m 45 -168 \C 0 8
      8 6 0 7 \S \ov 22 -39 16 16 \S \ov 15 -75 16 16 \S \re 7 -111 16
      16 \S \ov 37 -111 16 16 \S \re 15 -147 16 16 \S \ov 45 -147 16
      16 \S
    \end{sdpgf}&
    \begin{sdpgf}{0}{8}{75}{-178}{1.0pt}
      \begin{pgfscope}
        \pgfsetcolor{red}
        \re 28 -175 5 10 \S
        \re 43 -175 5 10 \S
        \re 13 -149 20 20 \S
        \re 43 -149 20 20 \S
        \re 5 -113 20 20 \S
        \re 35 -113 20 20 \S
        \re 65 -108 5 10 \S
        \re 13 -77 20 20 \S
        \re 43 -72 5 10 \S
        \re 58 -72 5 10 \S
        \re 20 -41 20 20 \S
        \re 50 -36 5 10 \S
        \re 28 -5 5 10 \S
        \re 43 -5 5 10 \S
      \end{pgfscope}
      \begin{pgfscope}
        \pgfsetroundcap
        \m 23 -59 \L 1 22 \S
        \m 58 -133 \L 10 25 \S
        \m 68 -98 \L -23 26 \S
        \m 45 -62 \L -9 25 \S
        \m 30 -23 \L 0 20 \S
        \m 51 -97 \L 9 25 \S
        \m 60 -62 \L -7 26 \S
        \m 53 -26 \L -8 23 \S
        \m 15 -95 \L 2 22 \S
        \m 39 -97 \L -11 24 \S
        \m 47 -133 \L -32 22 \S
        \m 23 -131 \L 22 20 \S
        \m 30 -168 \L -7 21 \S
        \m 45 -168 \L 8 21 \S
      \end{pgfscope}
    \end{sdpgf}
    \cr
    Network& Layout\crcr
%   \end{tabular*} 
  }
  
  \caption{A network drawing and its layout}
\end{figure}


\begin{definition}
  A \DefOrd{layout} for a graded network $(G,\ell)$ assigns
  \begin{itemize}
    \item
      to each level $k$ from $\ell(0)$ to $\ell(1)$ inclusive: a 
      vertical coordinate $y_k$;
    \item
      to each pair $(i,k)$, where \(i \in L_k\) is an item in level 
      $k$: a horizontal coordinate $x_{(i,k)}$;
    \item
      to each link $q$: a head endpoint $(x'_q, y'_q)$ and a tail 
      endpoint $(x''_q, y''_q)$.
  \end{itemize}
  A \DefOrd{layout scheme} furthermore imposes various conditions on a 
  layout, particularly that the vector from a link endpoint to the 
  corresponding item may depend only on the item type. More concrete 
  examples of such conditions can be:
  \begin{enumerate}
    \item \label{Biv1:Schema}
      If the head item of $(e,k)$ is \(v=h(e)\), then $x_{(v,k)} - 
      x'_{(e,k)}$ and $y_k - y'_{(e,k)}$ are determined by the 
      decoration $D(v)$ and head index $g(e)$. If instead the head 
      item of $(e,k)$ is $e$, then \(x_{(v,k)} = x'_{(e,k)}\) and 
      $y_k - y'_{(e,k)}$ is independent of $k$ and $e$.
    \item \label{Biv2:Schema}
      Similarly, if the tail item of $(e,k)$ is \(v=t(e)\), then 
      $x_{(v,k+1)} - x''_{(e,k)}$ and $y_{k+1} - y''_{(e,k)}$ are 
      determined by the decoration $D(v)$ and tail index $s(e)$. If 
      instead the tail item of $(e,k)$ is $e$, then \(x_{(v,k+1)} = 
      x''_{(e,k)}\) and $y_{k+1} - y''_{(e,k)}$ is independent of $k$ 
      and $e$.
    \item \label{Biv3:Schema}
      If \(i,j \in L_k\) are distinct, then \(\norm{ x_{(i,k)} - 
      x_{(j,k)} } \geqslant \delta > 0\), where $\delta$ may depend on 
      $i$ and $j$.
    \item
      Similarly, \(y_{k+1} - y_k \geqslant \mathit{\Delta y}_k\), where 
      $\mathit{\Delta y}_k$ may depend on the items in $L_k$Â and 
      $L_{k+1}$.
  \end{enumerate}
\end{definition}


Basically, the problem to be studied in this paper is to minimise the 
objective function
\begin{equation*}
  \sum_{q \in Q(G,\ell)} \Bigl( 
    (x'_q - x''_q)^2 + (y'_q - y''_q)^2 
  \Bigr) =
  \sum_{q \in Q(G,\ell)} (x'_q - x''_q)^2 +
  \sum_{q \in Q(G,\ell)} (y'_q - y''_q)^2
\end{equation*}
over the set of all layouts satisfying a given layout scheme\Ldash to 
minimise the square-sum of all link lengths\Rdash but since there is 
no direct connection between the $x$- and $y$-coordinate parts of this 
problem (unless one is introduced by the layout scheme), the actual 
problem to be studied is rather that of minimising the objective 
function
\begin{equation} \label{Eq0:Malfunktion}
  F(\mc{L}) = \sum_{q \in Q(G,\ell)} (x'_q - x''_q)^2
\end{equation}
of the layout $\mc{L}$ for a particular layout scheme. This scheme 
will be the \emDefOrd{tight} layout scheme, which essentially consists 
of conditions \ref{Biv1:Schema} and \ref{Biv2:Schema} above, coupled 
with a slightly strengthened form of \ref{Biv3:Schema} to the effect 
that the items in each level are tightly packed; at least $\norm{L_k}-1$ 
of the inequalities for level $k$ are in fact equalities.

Under the tight layout scheme, the free variables of the problem are 
reduced to the following:
\begin{itemize}
  \item
    For each level, one reference position $x_k$, which below will be 
    taken to be the left edge of the leftmost item. It furthermore 
    makes sense to fix \(x_{\ell(0)} = 0\), as the origin for the 
    coordinate system is irrelevant for how natural a layout looks.
  \item
    For each level, an ordering of $L_k$, which can be encoded as a 
    bijection \(\sigma_k\colon [m_k] \Fpil L_k\) where \(m_k = 
    \norm{L_k}\); the idea is that $\sigma_k(j)$ is the $j$th item 
    (counting from the left) in level $k$.
\end{itemize}
The relation between these variables and the ones of a general layout 
are that
\begin{align}
  x_{(\sigma(1),k)} ={}& x_k + \tfrac{1}{2} w_{\sigma(1)} \text{,}\\
  x_{(\sigma(j),k)} ={}& x_{(\sigma(j-1),k)} + 
    \tfrac{1}{2} w_{\sigma(j-1)} + \tfrac{1}{2} w_{\sigma(j)}
    \quad\text{for \(j>1\),}
\end{align}
where $w_i$ for \(i \in L_k\) denotes ``the width of item $i$''\Dash 
a design parameter in the layout scheme. Normally, $w_i$ will have 
one value if \(i \in E\) and be determined by $D(i)$ if $i$ is a 
vertex.

Since \eqref{Eq0:Malfunktion} is a sum of squares, one would expect 
it it have a global minimum, and under the fairly mild connectivity 
condition that there is at least one link between any two 
neighbouring levels, it is indeed possible to prove this. One 
argument goes that for any value \(v > 0\) attained by the objective 
function, it is possible to find an $r$ such that every layout 
with at least one coordinate of absolute value greater than $r$ must 
also have a value larger than $v$. The set of layouts with all 
coordinates being $\leqslant r$ in absolute value is compact, so the 
objective function has a minimum on it, and since that minimum must 
be $\leqslant v$ it must also be a global minimum of the objective 
function.



\subsection{Matrix formulation of the objective function}

Writing \(\vek{x}' = (x'_q)_{q \in Q(G,\ell)}\) and \(\vek{x}'' = 
(x''_q)_{q \in Q(G,\ell)}\), the objective function is simply 
$(\vek{x}' -\nobreak \vek{x}'')^2$. Now, since
\begin{equation}
  x'_q = x_{(i,k)} + \mathit{\Delta x}'_q
  \quad\text{and}\quad
  x''_q = x_{(j,k+1)} + \mathit{\Delta x}''_q
\end{equation}
where $(i,k)$ and $(j,k +\nobreak 1)$ are the head and tail items 
respectively of the link $q$, and $\mathit{\Delta x}'_q$ and 
$\mathit{\Delta x}''_q$ are constants, it follows that
\begin{equation}
  (\vek{x}' - \vek{x}'')^2 =
  (N' \vek{x} - N''\vek{x} + \vek{\Delta x})^2
\end{equation}
where \(\vek{x} = (x_{(i,k)})_{i \in L_k, k \in \Z}\), 
\(\vek{\Delta x} = (\mathit{\Delta x}'_q -\nobreak 
\mathit{\Delta x}''_q)_{q \in Q(G,\ell)}\), and the two matrices $N'$ 
and $N''$ are defined by
\begin{align*}
  N'_{(e,k)(i,l)} ={}& \begin{cases}
    1& \text{if $(i,l)$ is the head item of $(e,k)$,}\\
    0& \text{otherwise,}
  \end{cases}\\
  N''_{(e,k)(i,l)} ={}& \begin{cases}
    1& \text{if $(i,l)$ is the tail item of $(e,k)$,}\\
    0& \text{otherwise.}
  \end{cases}
\end{align*}
It would at this point be possible to combine $N'$ and $N''$ into one 
matrix $N'-N''$ with entries from $\{-1,0,1\}$ (exactly one $1$ and 
one $-1$ in each row), but for the continuation it is more convenient 
to split the vector $\vek{x}$ into separate parts for each level. 
Thus defining
\begin{align*}
  \vek{x}_k ={}& (x_{(i,k)})_{i \in L_k} \text{,}\\
  \vek{\Delta x}_k ={}& (\mathit{\Delta x}'_{(e,k)} - 
    \mathit{\Delta x}''_{(e,k)} )_{(e,k) \in Q(G,\ell)} \\
  (N'_k)_{e,i} ={}& \begin{cases}
    1& \text{if $(i,k)$ is the head item of $(e,k)$,}\\
    0& \text{otherwise,}
  \end{cases}\\
  (N''_k)_{e,i} ={}& \begin{cases}
    1& \text{if $(i,k+1)$ is the tail item of $(e,k)$,}\\
    0& \text{otherwise,}
  \end{cases}
\end{align*}
one gets
\begin{equation}
  (\vek{x}' - \vek{x}'')^2 = \sum_{k=\ell(0)}^{\ell(1)-1} 
  (N'_k \vek{x}_k - N''_k \vek{x}_{k+1} + \vek{\Delta x}_k)^2
  \text{.}
\end{equation}

The next step is to express $\vek{x}_k$ in terms of the reference 
position $x_k$, permutation $\sigma_k$, and width vector \(\vek{w}_k 
= (w_i)_{i \in L_k}\). Defining the matrix counterpart $P_k$ of 
$\sigma_k$ through
\begin{equation}
  (P_k)_{i,j} = \begin{cases}
    1& \text{if \(i = \sigma(j)\),}\\
    0& \text{otherwise,}
  \end{cases}
\end{equation}
it however turns out that $P_k^{-1} \vek{w}_k$ is a vector with the 
width of the leftmost item in the first position, the second leftmost 
item in the second position, etc. Hence the vector of \emph{positions} 
of the horizontally first, second, third, etc. items is $x_k\vek{1} + 
M P_k^{-1} \vek{w}_k$, where $\vek{1}$ denotes the vector with all 
elements equal to $1$, and $M$ is the matrix on the form
\begin{equation}
  M = \begin{pmatrix}
    \frac{1}{2} & 0 & 0 & \ldots & 0 \\
    1 & \frac{1}{2} & 0 & \ldots & 0 \\
    1 & 1 & \frac{1}{2} & \ldots & 0 \\
    \vdots& \vdots& \vdots& \ddots& \vdots \\
    1 & 1 & 1 & \ldots & \frac{1}{2}
  \end{pmatrix}
\end{equation}
i.e., all elements on the main diagonal are $\frac{1}{2}$, all above 
it are $0$, and all below it are $1$. This implies
\begin{equation}
  \vek{x}_k = x_k \vek{1} + P_k M P_k^{-1} \vek{w}_k
\end{equation}
and hence
\begin{equation} \label{Eq3:Malfunktion}
  (\vek{x}' - \vek{x}'')^2 = \sum_{k=\ell(0)}^{\ell(1)-1} 
  \Bigl( (x_k-x_{k+1})\vek{1} + N'_k P_k M P_k^{-1} \vek{w}_k 
    - N''_k P_{k+1} M P_{k+1}^{-1} \vek{w}_{k+1} + \vek{\Delta x}_k
  \Bigr)^2
\end{equation}
wherein the variables are the $x_k$ and $P_k$ for 
\(k=\ell(0),\dotsc,\ell(1)\). The constant vectors $\vek{w}_k$ and 
$\vek{\Delta x}_k$ are have numerical values that combine layout 
parameters according to the underlying graded network, whereas the 
constant matrices $N'_k$ and $N''_k$ are some manner of incidence 
matrices independent of such parameters, and $M$ only has to adjust 
its sides to match the network at hand.


\subsection{The choice of matrix group}

The point of the above exercise is to relax the problem so that 
instead of having $P_k$ be a permutation matrix, it ranges over some 
continuous matrix group, in which case continuous methods can be used 
to optimise it. However, this begs the question of \emph{which} matrix 
group one should use, since there are quite a lot of them.

Intuition suggests that a group with very flexible elements may be 
inappropriate, on the suspicion that they might allow difficulties to 
be swept under the rug rather than solved, and thereby not produce 
continuous solutions that are anywhere near a solution to the 
discrete problem. Indeed, if one takes
\begin{equation*}
  P = \begin{pmatrix}
    \sinh t & -\cosh t \\ \cosh t & -\sinh t
  \end{pmatrix}
  \text{,}\quad\text{thus making}\quad
  P^{-1} = \begin{pmatrix}
    -\sinh t & \cosh t \\ -\cosh t & \sinh t
  \end{pmatrix}
\end{equation*}
then for a uniform width vector \(\vek{w} = \transpose{(1,1)}\)
\begin{align*}
  P M P^{-1} \vek{w} ={}&
  \begin{pmatrix}
    \sinh t & -\cosh t \\ \cosh t & -\sinh t
  \end{pmatrix}
  \begin{pmatrix}
    \frac{1}{2} & 0 \\ 1 & \frac{1}{2}
  \end{pmatrix}
  \begin{pmatrix}
    -\sinh t & \cosh t \\ -\cosh t & \sinh t
  \end{pmatrix}
  \begin{pmatrix} 1 \\ 1 \end{pmatrix} 
  = \\ ={}&
  \frac{1}{2}
  \begin{pmatrix}
    \sinh t - 2\cosh t & -\cosh t \\ \cosh t - 2\sinh t & -\sinh t
  \end{pmatrix}
  \begin{pmatrix} e^{-t} \\ -e^{-t} \end{pmatrix} 
  = \\ ={}&
  \frac{e^{-t}}{2}
  \begin{pmatrix}
    \sinh t - \cosh t \\ \cosh t - \sinh t 
  \end{pmatrix}
  =
  \frac{e^{-2t}}{2}
  \begin{pmatrix} -1 \\ 1 \end{pmatrix} 
  \Lpil
  \vek{0} \text{ as \(t \Lpil \infty\).}
\end{align*}
Since the width of items is the primary source of link length 
(horizontally), effectively nullifying it as these matrices do is 
highly undesirable; the difficulty is being folded away rather than 
handled. Since this \(P \in \mathrm{SL}(n)\), both it and 
$\mathrm{GL}(n)$ are out as candidate groups.
Another problem that this example highlights is the lack of 
convergence, as far as the optimum point is concerned: as \(t \Lpil 
\infty\), the elements of $P$ diverge, even though $P M P^{-1} \vek{w}$ 
approaches $\vek{0}$. \emph{Without ever reaching it!} The infimum is 
never attained, because it lies at infinity.

Clearly the matrix group should be chosen so that the continuous 
optimisation problem really has an optimum. Topologically, this means 
it should be compact, like the orothogonal group $\mathrm{O}(n)$ or 
unitary group $\mathrm{U}(n)$; in fact, these two are the largest 
compact matrix groups in $\R^{n \times n}$ and $\C^{n \times n}$ 
respectively. They furthermore have the desirable property that they 
preserve vector length, which appears to be a major part of what went 
wrong in the example; if $P$ (and thus $P^{-1}$) preserves vector 
length, then \(\Norm{M^{-1}}^{-1} \norm{\vek{w}} \leqslant 
\norm{P M P^{-1} \vek{w}} \leqslant \Norm{M} \norm{\vek{w}}\).

Another topological hurdle in the choice of matrix group is that it 
must be connected, and unfortunately this rules out all real matrix 
groups. The simple fact is that \(\det P = 1\) for all even 
permutations $P$, whereas \(\det P = -1\) for all odd permutations 
$P$, and every continuous path between $1$ and $-1$ in $\R$ passes 
through $0$, which can not be the determinant of any invertible 
matrix; no real matrix group contains a continuous path between an 
even and an odd permutation! Hence there is no choice but to go 
complex (where $-1$ and $1$ can be joined along the unit circle), which 
in particular implies the square $\vek{u}^2$ in \eqref{Eq3:Malfunktion} 
includes a conjugation $\vek{u}^* \vek{u}$ rather than being just 
$\transpose{\vek{u}}\vek{u}$.

Now, while unitary $P$ must preserve vector length, they can still 
rotate vectors arbitrarily, which is perhaps not entirely natural for 
a vector of \emph{widths} like $\vek{w}$. Taking \(\vek{w} = 
\transpose{(1,1,1)}\) and \(P = \left( \begin{smallmatrix} 1 & 0 & 0 
\\ 0 & 0 & 1 \\ 0 & -1 & 0 \end{smallmatrix} \right)\), one gets
\begin{multline*}
  P M P^{-1} \vek{w} =
  \begin{pmatrix} 1&0&0 \\ 0&0&1 \\ 0&-1&0 \end{pmatrix}
  \begin{pmatrix} \frac{1}{2} & 0 & 0\\ 1 & \frac{1}{2} & 0 \\
    1 & 1 & \frac{1}{2} \end{pmatrix}
  \begin{pmatrix} 1&0&0 \\ 0&0&-1 \\ 0&1&0 \end{pmatrix}
  \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}
  = \\ =
  \begin{pmatrix} 1&0&0 \\ 0&0&1 \\ 0&-1&0 \end{pmatrix}
  \begin{pmatrix} \frac{1}{2} & 0 & 0\\ 1 & \frac{1}{2} & 0 \\
    1 & 1 & \frac{1}{2} \end{pmatrix}
  \begin{pmatrix} 1 \\ -1 \\ 1 \end{pmatrix}
  = 
  \begin{pmatrix} 1&0&0 \\ 0&0&1 \\ 0&-1&0 \end{pmatrix}
  \begin{pmatrix} \frac{1}{2} \\ \frac{1}{2} \\ \frac{1}{2} \end{pmatrix}
  = 
  \begin{pmatrix} \frac{1}{2} \\ \frac{1}{2} \\ -\frac{1}{2} \end{pmatrix}
\end{multline*}
which does not seem a sensible set of midpoints for three disjoint 
items of width $1$; this rather corresponds to a width vector 
$(1,-1,-1)$. Apparently such rotations are not much like the 
permutations that $P$ is supposed to approximate, and could again be 
used to sweep difficulties under the rug, but how might one prohibit 
the continuous problem from venturing out into points so far from any 
permutation? One idea could be to require that the total width of a 
level should stay the same; obviously that must in the original 
discrete problem be true for all permutations of the items in a level.

The total width of a level with width vector $\vek{w}$ is 
$\transpose{\vek{1}} \vek{w}$, so the condition that this stays the 
same would be that \(\transpose{\vek{1}} P^{-1} \vek{w} = 
\transpose{\vek{1}} \vek{w}\) for all $\vek{w}$. Since realistic 
width vectors still are general enough that a basis for $\C^n$ can 
consist entirely of these, it follows that \(\transpose{\vek{1}} P^{-1} 
= \transpose{\vek{1}}\), or equivalently \(\transpose{\vek{1}} P^* = 
\transpose{\vek{1}}\), which after conjugation becomes \(\vek{1} = 
P \vek{1}\); in other words, the (unitary) $P$ that preserve total 
width are precisely those that map the all-ones vector $\vek{1}$ to 
itself. Those matrices form a 
subgroup of $\mathrm{U}(n)$, which may be denoted $\UOne(n)$. Letting 
$P$ range over $\UOne(n)$ rather than the whole of $\mathrm{U}(n)$ 
will likely give a better approximation of the discrete problem.

Restricting to a subgroup does however reraise the question of 
whether there is a continuous path from one permutation to another. 
In this case, that question is easily answered by considering the 
matrix family
\begin{equation} \label{Eq:KontTransposition}
  P(t) 
  = 
  \frac{1}{2}
  \begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix}
  \begin{pmatrix} 1 & 0  \\ 0 & e^{it} \end{pmatrix}
  \begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix}
  =
  \frac{1}{2}
  \begin{pmatrix}
    1 + e^{it} & 1 - e^{it} \\
    1 - e^{it} & 1 + e^{it}
  \end{pmatrix}
\end{equation}
which has \(P(0) = \left( \begin{smallmatrix} 1&0 \\ 0&1 
\end{smallmatrix} \right)\) and \(P(\pi) = \left( 
\begin{smallmatrix} 0&1 \\ 1&0 \end{smallmatrix} \right)\). These are 
unitary since both \(H = \frac{1}{\sqrt{2}} \left( 
\begin{smallmatrix} 1&1 \\ 1&-1 \end{smallmatrix} \right)\) and 
\(R_t = \left( \begin{smallmatrix} 1&0 \\ 0&\exp(it) 
\end{smallmatrix} \right)\) are unitary\Ldash the former having \(H^* 
H = H^2 = I\)\Rdash and keep $\vek{1}$ fixed since 
\(H R_t H \transpose{(1,1)} = H R_t \transpose{(\sqrt{2},0)} = 
H \transpose{(\sqrt{2},0)} = \transpose{(1,1)}\). Since any 
permutation can be transformed to any other permutation by 
composition with some number of transpositions, and since each 
transposition is part of such a continuous subgroup, it follows that 
all permutations belong to the same component of $\UOne(n)$. 
\textbf{Therefore} $\UOne(m_k)$ shall be the domain over which $P_k$ 
ranges.

At this point, it may be instructive to clarify how the corresponding 
layout changes when $P$ moves along the curve 
\eqref{Eq:KontTransposition} in $\UOne(2)$. Taking \(\vek{w} = 
\transpose{(1,1)}\), the position vector for \(\left( \begin{smallmatrix} 
1&0 \\ 0&1 \end{smallmatrix} \right) = P(0)\) is 
$\transpose{(\frac{1}{2}, \frac{3}{2})}$, for \(\left( \begin{smallmatrix} 
0&1 \\ 1&0 \end{smallmatrix} \right) = P(\pi)\) it is 
$\transpose{(\frac{3}{2}, \frac{1}{2})}$, and for \(P(\pi/2) = 
\frac{1}{2} \left( \begin{smallmatrix} 1+i& 1-i \\ 1-i& 1+i 
\end{smallmatrix} \right)\) it is $\transpose{( 1 -\nobreak 
\frac{i}{2}, 1 +\nobreak \frac{i}{2} )}$. In other words, the items 
pass beside each other rather than through, adding separation in the 
imaginary direction to compensate for lack of separation in the real 
direction. This renders the items sort-of solid, without making their 
arrangement rigid!

A cautioning question that this example raises is however that of how 
an algorithm would decide which item goes into the upper half-plane 
of $\C$ and which item goes into the lower. Since there is nothing in 
the objective function $F$ which distinguishes between $i$ and $-i$, 
both choices must be equal. In particular it turns out that any purely 
real $P$ must be a critical point for the objective function, as there 
by symmetry of $i$ and $-i$ can be no imaginary component of the 
derivative of the objective function there, and on the other hand the 
tangent space of $\UOne(n)$ (as shown in the next subsection) is 
purely imaginary at such points! One might suspect that most 
permutation $P$s are in fact saddle points, which makes the problem 
of finding a good descent direction there somewhat intricate.




\subsection{The domain as a Lie group}

The effect of the restrictions above is that the optimisation domain 
is a manifold with the structure
\begin{equation}
  \mc{M} = 
%   \prod_{k=\ell(0)}^{\ell(1)} \bigl( \R \times \UOne(m_k) \bigr) \cong
  \R^{\ell(1)-\ell(0)} \times \prod_{k=\ell(0)}^{\ell(1)} \UOne(m_k)
  \text{.}
\end{equation}
The vector space part here is straightforward, but the $\UOne(m_k)$ 
factors require some thought. What are the directions in which one 
can move in such a space? These are what make up the \emph{tangent 
space} of the manifold.

A powerful tool for studying tangent spaces of algebraic sets is the 
epsilon formalism of algebraic geometry (not to be confused with the 
epsilon--delta formalism of analysis). Basically the idea is that if 
calculations are normally carried out over some ring $\mc{R}$, then 
they should instead be carried out over the ring
\begin{equation}
  \mc{R}[\ve \mathop{\vert} \ve^2 = 0] =
  \setOf{ a + \ve b }{ a,b \in \mc{R} } \text{,}
\end{equation}
where the $\ve$-coefficient is interpreted as some manner of 
derivative of the quantity in the first term. The advantage of this 
is that the rules of differentiation become corollaries of rules of 
algebra, for example the product rule pops out from
\begin{equation*}
  (a + \ve a') (b + \ve b') =
  ab + \ve ab' + \ve a'b + \ve^2 a'b' =
  ab + \ve (ab' + a'b)
\end{equation*}
since \(\ve^2 = 0\), and therefore no special derivation of 
differentiation rules for e.g.~vectors or matrices is required.

Considering first the situation at \(I \in \UOne(n)\), one can see 
that the matrix $A$ is a tangent at $I$ if and only if \(I + \ve A\) 
satisfies the defining equations for $\UOne(n)$, i.e., 
\begin{align*}
  I ={}& (I + \ve A)^* (I + \ve A) = I + \ve A + \ve A^*
    & \Epil & A^* = -A \text{,} \\
  \vek{1} ={}& (I + \ve A) \vek{1} = \vek{1} + \ve A \vek{1}
    & \Epil & A \vek{1} = \vek{0} \text{.}
\end{align*}
An $A$ satisfying \(A^* = -A\) is said to be anti-Hermitian. 
Alternative presentations of it are \(A = iB\) where \(B \in \C^{n 
\times n}\) is Hermitian (\(B^* = B\)) and \(A = C + iD\) where \(C,D 
\in \R^{n \times n}\) are antisymmetric and symmetric respectively, 
i.e., \(\transpose{C} = -C\) and \(\transpose{D} = D\). The most 
common presentation below will be the $iB$ one, but for determining 
the effect of \(A \vek{1} = \vek{0}\) it seems $C + iD$ is more 
convenient.

Obviously \(A = C + iD\) satisfies \(A \vek{1} = \vek{0}\) if and 
only if \(C \vek{1} = \vek{0}\) and \(D \vek{1} = \vek{0}\)\Dash two 
conditions that merely amount to the row sums all being $0$. It is 
easy to see that $D$ has $\binom{n}{2}$ degrees of freedom: put 
anything you like above the diagonal ($\binom{n}{2}$ entries), then 
what goes below the diagonal is determined by symmetry, and after 
that the row sum conditions force what goes into the diagonal. It is 
slightly trickier to see that $C$ has $\binom{n-1}{2}$ degrees of 
freedom, but it is clear that it must have the structure
\begin{equation*}
  \begin{pmatrix}
    0& c_{1,2} & c_{1,3}& \ldots& c_{1,n-1}& -s_1\\
    -c_{1,2}& 0& c_{2,3}& \ldots& c_{2,n-1}& -s_2\\
    -c_{1,3}& -c_{2,3}& 0& \ldots& c_{3,n-1}& -s_3\\
    \vdots& \vdots& \vdots& \ddots& \vdots& \vdots\\
    -c_{1,n-1}& -c_{2,n-1}& -c_{3,n-1}& \ldots& 0& -s_{n-1} \\
    s_1& s_2& s_3& \ldots& s_{n-1}& 0
  \end{pmatrix}
\end{equation*}
where \(s_k = \sum_{j=1}^{n-1} c_{k,j} = \sum_{1 \leqslant j < k} 
-c_{k,j} + \sum_{k < j < n} c_{k,j}\). Hence the rest is 
determined by $c_{k,l}$ for \(1 \leqslant k < l < n\), and therefore 
there can be at most $\binom{n-1}{2}$ degrees of freedom. Conversely, 
any choice of $c_{k,l}$ for \(1 \leqslant k < l < n\) will give rise 
to an antisymmetric matrix as shown above, where rows $1$ through 
$n-1$ sum to $0$. Hence it is only the last row sum that might impose 
an additional restriction, but
\begin{multline*}
  \sum_{k=1}^{n-1} s_k = 
  \sum_{k=1}^{n-1} \sum_{j=1}^{n-1} c_{k,j} =
  \sum_{1 \leqslant k < j < n} c_{k,j} + 
    \sum_{1 \leqslant j < k < n} c_{k,j} = \\ =
  \sum_{1 \leqslant k < j < n} ( c_{k,j} + c_{j,k} ) =
  \sum_{1 \leqslant k < j < n} ( c_{k,j} - c_{k,j} ) =
  0
\end{multline*}
and so the final row sum will be $0$ anyway, meaning there are 
exactly $\binom{n-1}{2}$ degrees of freedom. Together, \(\binom{n}{2} 
+ \binom{n-1}{2} = (n -\nobreak 1)^2\), so it follows that the 
manifold $\UOne(n)$ is $(n {-} 1)^2$-dimensional.

Returning to the example of a network with $5$ levels of $7$ items 
each, this implies that the manifold $\mc{M}$ in that case is 
$184$-dimensional. While large, it is not unmanagably large. 
Fully searching such a space is not feasible, but hopefully it will 
be sufficient to search in only one direction (at a time), namely 
that of the gradient of the objective function.

For general tangent spaces\Ldash meaning $T_P\UOne(n)$ for a general 
\(P \in \UOne(n)\)\Rdash it would be possible to make the ansatz $P + 
\ve A$, but more convenient to take $P(I +\nobreak \ve A)$, as
\begin{multline*}
  I = 
  \bigl( P (I + \ve A) \bigr)^* P (I+ \ve A) =
  (I + \ve A^*) P^* P (I+ \ve A) = \\ =
  (I + \ve A^*) (I+ \ve A) =
  I + \ve(A^*+A)
\end{multline*}
again gives \(A^* = -A\), and similarly \(\vek{1} = 
P(I +\nobreak \ve A)\vek{1} = \vek{1} + \ve P A \vek{1}\) again gives 
\(A \vek{1} = \vek{0}\). In other words, that $P$ factor in this 
ansatz renders all tangent spaces numerically the same, even though 
they are geometrically distinct.

Suppose now that we are in the point \(P_0 \in \UOne(n)\) and have 
somehow determined that our search direction is $P_0 A$ for some 
anti-Hermitian \(A \in \C^{n \times n}\) with \(A\vek{1} = \vek{0}\)\Dash 
then which are the actual points we are going to look at next? Presumably 
those along the curve
\begin{equation}
  P(t) = P_0 \exp(tA) = P_0 \sum_{k=0}^\infty \frac{(tA)^k}{k!}
\end{equation}
since the exponential map \emph{is} the standard way of mapping 
tangents to points. It is clear that \(P(0 + \nobreak \ve) = 
P_0 \exp(\ve A) = P_0 (I +\nobreak \ve A)\), so this curve indeed has 
the right tangent at $P_0$. Moreover
\begin{multline*}
  \bigl( P_0 \exp(tA) \bigr)^* =
  \biggl( P_0 \sum_{k=0}^\infty \frac{(tA)^k}{k!} \biggr)^* =
  \biggl( \sum_{k=0}^\infty \frac{(t A^*)^k}{k!} \biggr) P_0^* = \\ =
  \biggl( \sum_{k=0}^\infty \frac{(-t A)^k}{k!} \biggr) P_0^{-1} =
  \exp(-tA) P_0^{-1} =
  \bigl( P_0 \exp(tA) \bigr)^{-1} \text{,}
\end{multline*}
which shows that the curve stays within $\mathrm{U}(n)$. That it also 
stays within $\UOne(n)$ follows from
\begin{multline*}
  P_0 \exp(tA) \vek{1} =
  P_0 \biggl( I + \sum_{k=1}^\infty \frac{(tA)^k}{k!} \biggr) \vek{1} 
  =
  P_0 \vek{1} + 
    t \biggl( \sum_{k=1}^\infty \frac{(tA)^{k-1}}{k!} \biggr) A \vek{1} 
    = \\ =
  \vek{1} +
    t \biggl( \sum_{k=1}^\infty \frac{(tA)^{k-1}}{k!} \biggr) \vek{0} 
    =
  \vek{1} \text{.}
\end{multline*}

In the $\R^n$ component of the manifold $\mc{M}$, a tangent is 
instead an ordinary $\R^n$ vector $\vek{v}$, and the exponential map 
is simply the scalar multiple \(\exp(t\vek{v}) = t\vek{v}\), as the 
group operation there is ordinary vector addition.

In summary, a \emph{point} in the manifold $\mc{M}$ is a tuple
\begin{equation*}
  p = (\vek{u}, P_{\ell(0)}, \dotsc, P_{\ell(1)})
\end{equation*}
where \(\vek{u} \in \R^{\ell(1)-\ell(0)}\) and \(P_k \in \UOne(m_k)\) 
for \(k=\ell(0),\dotsc,\ell(1)\). A \emph{tangent} at this point is a 
tuple
\begin{equation*}
  (\vek{v}, i P_{\ell(0)} B_{\ell(0)}, \dotsc, 
    i P_{\ell(1)} B_{\ell(1)})
\end{equation*}
where \(\vek{v} \in \R^{\ell(1)-\ell(0)}\) and \(B_k \in \C^{m_k 
\times m_k}\) for \(k=\ell(0),\dotsc,\ell(1)\) are Hermitian. The 
point $p(t)$ of the integral curve which has this tangent at \(p = 
p(0)\) is
\begin{equation*}
  p(t) = \bigl( \vek{u} + t\vek{v}, P_{\ell(0)} \exp(itB_{\ell(0)}), 
    \dotsc, P_{\ell(1)} \exp(itB_{\ell(1)}) \bigr)
  \text{.}
\end{equation*}
The manifold $\mc{M}$ is moreover a Lie group, with group operation
\begin{equation*}
  (\vek{u}, P_{\ell(0)}, \dotsc, P_{\ell(1)}) \cdot
  (\vek{v}, Q_{\ell(0)}, \dotsc, Q_{\ell(1)}) :=
  (\vek{u} + \vek{v}, P_{\ell(0)} Q_{\ell(0)}, \dotsc, 
    P_{\ell(1)} Q_{\ell(1)})
\end{equation*}
as given by its presentation as a Cartesian product of Lie groups. 
This operation is useful for stepwise motion in the manifold, as it 
is quite a bit cheaper to compute than the exponent.


\subsection{Gradients and the Riemannian structure}

Given an algebraic function $f(P)$, it is easy to substitute $P(I 
+\nobreak i \ve B)$ for $P$ and extract the $\ve$-term, but this will 
\emph{not} be the gradient of $f$ at $P$. A simple consideration of 
types will reveal why: a gradient is a tangent, but the $\ve$-term is 
a function which maps some tangent $B$ to a real number; it is thus a 
\emph{cotangent}. Tangents and cotangents (at a point) live in finite 
dimensional vector spaces that are dual to each other, so it is 
perfectly possible to convert differentials to gradients, but picking 
any particular scheme for doing this is equivalent to defining an 
inner product on the corresponding tangent space. Hence it should be 
better to start from an inner product that is known to be sensible 
and derive a tangent--cotangent (gradient--differential) 
correspondence from that.

Being embedded in $\C^{n \times n}$, it seems reasonable to give 
$T_P\UOne(n)$ an inner product that simply is a restriction of one on 
the whole of $\C^{n \times n}$; this will ensure it varies smoothly 
from point to point. What is furthermore the numerically most simple 
inner product on $\C^{n \times n}$ should be that which follows from 
the vector space isomorphism \(\C^{n \times n} \cong \R^{2n^2}\), 
namely to take the element-wise
\begin{equation}
  \langle A, B \rangle :=
  \sum_{k=1}^n \sum_{l=1}^n \bigl( \re(A_{kl}) \re(B_{kl}) +
    \im(A_{kl}) \im(B_{kl}) \bigr)
  \text{.}
\end{equation}
This choice is however not as hopelessly ad hoc as this coordinate 
presentation might suggest, since
\begin{multline}
  \sum_{k=1}^n \sum_{l=1}^n \bigl( \re(A_{kl}) \re(B_{kl}) +
    \im(A_{kl}) \im(B_{kl}) \bigr)
  =
  \sum_{k=1}^n \sum_{l=1}^n \re \bigl( \overline{A_{kl}} B_{kl} \bigr)
  = \\ =
  \sum_{l=1}^n \sum_{k=1}^n \re \bigl( (A^*)_{lk} B_{kl} \bigr)
  =
  \sum_{l=1}^n \re \bigl( (A^* B)_{ll} \bigr)
  =
  \re \Tr (A^* B) \text{.}
\end{multline}
This coordinate-free form also makes it easy to see that if \(iPB_1, 
iPB_2 \in T_P\UOne(n)\) are given then
\begin{multline*}
  \langle iPB_1, iPB_2 \rangle =
  \re \Tr \bigl( (iPB_1)^* (iPB_2) \bigr) =
  \re \Tr ( -i B_1^* P^* i P B_2 ) = \\ =
  \re \Tr ( B_1^* P^* P B_2 ) = 
  \re \Tr ( B_1^* B_2 ) =
  \langle B_1, B_2 \rangle \text{,}
\end{multline*}
meaning the inner product of two tangents represented by the 
Hermitian matrices $B_1$ and $B_2$ is independent of the point at 
which these tangents are found.

For a finite-dimensional inner product space $V$ over $\R$ in 
general, the inner product \(\langle \cdot, \cdot \rangle \colon 
V \times V \Fpil \R\) gives a simple expression for the isomorphism 
\(\iota\colon V \Fpil V^*\), namely \(\iota(u)(v) = \langle u, 
v\rangle\) for all \(u,v \in V\); $\iota(u)$ is a linear map \(V 
\Fpil \R\) and thus by definition an element of $V^*$. What is needed 
here is however rather the inverse $\iota^{-1}\colon V^* \Fpil V$, 
and that is notationally a bit trickier. 



\end{document}
